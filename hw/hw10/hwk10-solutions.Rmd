---
title: "Homework 10"
output: html_document
---

```{css, echo = F}
.soln {
background-color: #E6E6FA;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE, error = TRUE,
                      fig.height = 3)
library(tidyverse)
```

```{r, echo=F}
old <- theme_get()

theme_set(theme(panel.background = element_rect(fill = "#ededfb")))
```


# Preliminaries

- This file should be in `STAT240/homework/hw10` on your local computer.
- Download `parental_leave.csv` and `happiness_2019` to `STAT240/data`.


# Problem 1

Continue working with the parental leave data from homework 9.

```{r}
leave <- read_csv("../../data/parental_leave.csv")

# Remove spaces from column names
names(leave) <- make.names(names(leave))

# Tidy numeric columns
leave <- leave %>%
  mutate_at(vars(Paid.Maternity.Leave, Unpaid.Maternity.Leave,
                 Paid.Paternity.Leave, Unpaid.Paternity.Leave),
            as.numeric) %>%
  replace(is.na(.), 0)
```

(a) Re-do the basic data tidying from homework 9 problem 5(a): Mutate a column for total paid leave by adding the hours of paid maternity and paternity leave. Optionally, give the columns shorter names that are easier to reference.

```{r, class.source="soln", class.output="soln"}
leave <- leave %>%
  rename(mat_paid = Paid.Maternity.Leave,
         pat_paid = Paid.Paternity.Leave,
         mat_unpaid = Unpaid.Maternity.Leave,
         pat_unpaid = Unpaid.Paternity.Leave) %>%
  mutate(total_paid = mat_paid + pat_paid)
```

(b) Consider comparing companies in "Educational Services: College & Universities" (ESCU) to companies in "Healthcare: Hospitals & Clinics" (HHC).  Build a 95% CI for the difference in average total paid leave and interpret the results.

```{r, class.source="soln", class.output="soln"}
escu_vs_hhc <- leave %>% 
  filter(Industry %in% c("Educational Services: College & Universities",
                         "Healthcare: Hospitals & Clinics")) %>% 
  group_by(Industry) %>% 
  summarize(avg_leave = mean(total_paid), sd_leave = sd(total_paid), n = n())

xbar1 <- escu_vs_hhc$avg_leave[1]
xbar2 <- escu_vs_hhc$avg_leave[2]
s1 <- escu_vs_hhc$sd_leave[1]
s2 <- escu_vs_hhc$sd_leave[2]
n1 <- escu_vs_hhc$n[1]
n2 <- escu_vs_hhc$n[2]
```

<span style="color:#5858d0">  After gathering data summaries, we need to calculate the approximate degrees of freedom to use for our T reference distribution. </span>

```{r, class.source="soln", class.output="soln"}
w_numer <- (s1^2/n1 + s2^2/n2)^2
w_denom <- (s1^4/(n1^2*(n1-1)) + s2^4/(n2^2*(n2-1)))

w <- w_numer / w_denom
```

<span style="color:#5858d0">  Now, we can find all of the components of our CI. </span>

```{r, class.source="soln", class.output="soln"}
pt_est <- xbar1 - xbar2
se <- sqrt(s1^2/n1 + s2^2/n2)

cv <- qt(0.975, df = w)

c(pt_est - cv*se, pt_est + cv*se)
```

<span style="color:#5858d0">  We are 95% confident that the true difference between average ESCU and HHC paid leave is within (1.509, 6.7). </span>


# Problem 2

Does the educational industry give more paid leave than the healthcare industry?  Perform a hypothesis test of 
$$H_0: \mu_{ESCU} - \mu_{HHC} = 0 \quad \text{versus}\quad H_A: \mu_{ESCU} - \mu_{HHC} > 0$$
with $\alpha = 0.05$.  Check your results with `t.test()`.

<span style="color:#5858d0">  Our test statistic is based on the difference in the observed means, and the null distribution is a T with $w$ degrees of freedom (found in problem 5). Our p-value is the area above our test statistic. </span>

```{r, class.source="soln", class.output="soln"}
se <- sqrt(s1^2/n1 + s2^2/n2)

test_stat <- ((xbar1 - xbar2) - 0)/se

test_stat

pt(test_stat, df = w, lower.tail = F)
```

<span style="color:#5858d0">  We get a very small p-value of 0.001, which is less than $\alpha = 0.05$.  We have evidence that the educational industry gives more paid leave than the healthcare industry, on average.  We get the same result with `t.test`. </span>

```{r, class.source="soln", class.output="soln"}
obs_escu <- leave %>%
  filter(Industry == "Educational Services: College & Universities") %>%
  pull(total_paid)

obs_hhc <- leave %>%
  filter(Industry == "Healthcare: Hospitals & Clinics") %>%
  pull(total_paid)

t.test(obs_escu, obs_hhc, alternative = "greater")
```


# Problem 3

Repeat the test in problem 2, but instead write hypotheses for $\mu_{HHC} - \mu_{ESCU}$ (the order of subtraction is switched).  Show that you get identical results to problem 2.

<span style="color:#5858d0">  When we reverse the order of subtraction, we need to reverse the direction of the alternative hypothesis.  If we expect ESCU to be the bigger group, then $\mu_{HHC} - \mu_{ESCU}$ should be negative.  We have:
$$H_0: \mu_{HHC} - \mu_{ESCU} \ge 0 \quad \text{versus} \quad H_A: \mu_{HHC} - \mu_{ESCU} < 0$$
</span>

<span style="color:#5858d0">  Our test statistic has the reverse order of subtraction, which gives us the same test statistic as in problem 2, but with the opposite sign.  Now, we calculate a p-value by looking at the area below our test statistic. </span>

```{r, class.source="soln", class.output="soln"}
test_stat <- ((xbar2 - xbar1) - 0)/se

test_stat

pt(test_stat, df = w)
```

<span style="color:#5858d0">  We get an identical p-value of 0.001. </span>


# Problem 4

Now, consider the "Technology: Software" industry.  Perform the appropriate hypothesis test to determine whether there is a difference in paid maternity and paternity leave within this industry.

<span style="color:#5858d0">  Because maternity and paternity leave are both decided within a company, we have paired data, and we should perform our analysis at the "company level".  First, calculate the difference in maternity - paternity paid leave for each software company. </span>

```{r, class.source="soln", class.output="soln"}
leave_differences <- leave %>%
  filter(Industry == "Technology: Software") %>%
  mutate(diffs = mat_paid - pat_paid) %>%
  pull(diffs)
```

<span style="color:#5858d0">  Now, we need to perform a one-sample T test.  Our hypotheses are
$$H_0: \mu_{diff} = 0 \quad \text{versus} \quad H_A: \mu_{diff} \neq 0$$
</span>

<span style="color:#5858d0">  Let's gather the summaries of the differences and calculate a test statistic and two-sided p-value. </span>

```{r, class.source="soln", class.output="soln"}
xbar <- mean(leave_differences)
s <- sd(leave_differences)
n <- length(leave_differences)

test_stat <- (xbar - 0) / (s / sqrt(n))

test_stat

pt(test_stat, df = n-1, lower.tail = F)
```

<span style="color:#5858d0">  We have an extremely small p-value, which corresponds to very strong evidence against the null.  We have evidence of a difference in the amount of paid maternity and paternity leave offered in the software industry. </span>


# Problem 5

Match the six values of correlation to the scatterplots in `p5_choices.png`. Briefly justify your choices.

- $r = -0.85$

<span style="color:#5858d0"> Graph C shows $r = -0.85$. There is a strong, but not perfect, negative correlation. </span>

- $r = -0.74$

<span style="color:#5858d0"> Graph D shows $r = -0.74$. There a negative correlation that is not quite as strong as shown in graph C. </span>

- $r = 0.08$

<span style="color:#5858d0"> Graph B shows $r = 0.08$.  There is no clear linear relationship between x and y, so correlation is approximately 0. </span>

- $r = 0.44$

<span style="color:#5858d0"> Graph F shows $r = 0.44$.  There is a visible positive trend between x and y, but it is not very strong. </span>

- $r = 0.98$

<span style="color:#5858d0"> Graph A shows $r = 0.98$.  There is a very strong positive relationship between x and y, but there is still a bit of random scatter present. </span>

- $r = 1$

<span style="color:#5858d0"> Graph E shows $r = 1$.  There is a perfect linear relationship with no random error. </span>


# Problem 6

The [Happiness index](https://www.kaggle.com/datasets/sougatapramanick/happiness-index-2018-2019) data contains information on the happiness index of different countries from 2018-2019.  We will focus on the happiness index (`Score`) and GDP per capita of countries in 2019.

```{r}
happiness <- read_csv("../../data/happiness_2019.csv")

# Remove spaces from column names
names(happiness) <- make.names(names(happiness))
```

(a) Create a scatterplot of GDP per capita (x) versus happiness index (y) and calculate the correlation between length and weight.  Comment on the strength and magnitude of the linear relationship, and whether a linear model seems to be appropriate

```{r, class.source="soln", class.output="soln"}
happiness %>%
  ggplot(aes(x = GDP.per.capita, y = Score)) +
  geom_point()

x <- happiness$GDP.per.capita
y <- happiness$Score

cor(x, y)
```

<span style="color:#5858d0"> Both the scatterplot and correlation indicate a fairly strong positive linear relationship.  Visually, the points are scattered around a line, so a linear model seems like a good fit for this data. </span>

(b) Calculate the slope and intercept of a least-squares linear regression model for GDP per capita versus happiness index.  Do this with the "by hand formulas" and check your work with R `lm()`.  Interpret the coefficients in context.

```{r, class.source="soln", class.output="soln"}
r <- cor(x, y)
xbar <- mean(x); ybar <- mean(y)
s_x <- sd(x); s_y <- sd(y)

est_slope <- r * (s_y / s_x)
est_intercept <- ybar - (xbar * est_slope)

c(est_slope, est_intercept)
```

<span style="color:#5858d0"> The linear model is $\hat{y}_i = 3.4 + 2.22(x_i)$.  3.4 is the therotical happiness score for a country with 0 GDP per capital, and 2.22 is the expected increase in happiness score for 1 unit increase in GDP per capita.  We get the same results with `lm()`. </span>

```{r, class.source="soln", class.output="soln"}
happiness_mod <- lm(Score ~ GDP.per.capita, data = happiness)

happiness_mod
```

(c) Perform a residual analysis of the happiness linear model to assess the fit.  Build a scatterplot of residuals and comment on the three assumptions:

- Linearity
- Normality
- Constant variance

```{r, class.source="soln", class.output="soln"}
happiness %>%
  mutate(residuals = resid(happiness_mod)) %>%
  ggplot(aes(x = GDP.per.capita, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

<span style="color:#5858d0"> Linearity: There are no curved patterns in the scatterplot of residuals, so the relationship between x and y appears to be linear. </span>

<span style="color:#5858d0"> Normality: Most of the points are close to the line, with a few points scattered away from the line in a symmetric way.  So, the residuals appear to be normal. </span>

<span style="color:#5858d0"> Constant variance: The scztter of the residuals around the line is roughly constant for all values of x. </span>


# Problem 7

Using your linear model, predict the happiness index for theoretical countries with 0.75, 1.25, and 2 GDP per capita.  Which of these predictions would you consider to be the least reliable, and why?

```{r, class.source="soln", class.output="soln"}
gdp <- c(0.75, 1.25, 2)

# Using predict function
predict(happiness_mod, newdata = tibble(GDP.per.capita = gdp))

# By hand
3.4 + (2.22 * gdp)
```

<span style="color:#5858d0"> The predictions for happiness index are approximately 5.065, 6.175, and 7.84.  I am most skeptical of the prediction at x = 2 GDP per capita, because that value is well outside the range of our original data.  We have only verified that the linear model is correct for values of x within [0, 1.684].  Predicting at a value of 2 is extrapolation. </span>


# Problem 8

What are the conclusions we can make about the relationship between GDP per capita and happiness index, based on the linear model?  Does money buy happiness?

<span style="color:#5858d0"> Based on our linear model, we can say that countries with higher GDP per capita generally tend to have a higher happiness index.  However, we cannot make any conclusions about causality between the two variables.  It is possible that high GDP and high happiness are both related to other aspects of the country (e.g. more stable government and more social support). </span>



